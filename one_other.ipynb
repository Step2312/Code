{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 心病辨证要点\n",
    "\n",
    "### 心气虚证：\n",
    "- 心悸怔忡与气虚症状共见\n",
    "- 气虚证：神疲乏力、少气懒言、脉虚、动则诸症加剧为主要表现\n",
    "- 心悸，胸闷，气短，精神疲倦，或有自汗，面色淡白，舌质淡，脉虚\n",
    "\n",
    "### 心血虚证：\n",
    "- 心悸、失眠、多梦与血虚症状共见\n",
    "- 血虚证：面、睑、唇、舌色淡白，脉细等为主要表现\n",
    "- 心悸、头晕眼花、失眠、多梦、健忘、面色淡白或萎黄、舌色淡、脉细无力\n",
    "\n",
    "### 心阴虚证\n",
    "- 心悸、心烦、失眠与虚热症状共现\n",
    "- 阴虚证：口咽干燥、五心烦躁、潮热盗汗、两颧潮红、舌红少苔、脉细数\n",
    "- 心烦、心悸，失眠，多梦，口燥咽干，形体消瘦，或见手足心热，潮热盗汗，两颧潮红，舌红少苔乏津，脉细数\n",
    "\n",
    "\n",
    "### 心阳虚证\n",
    "\n",
    "- 心悸怔忡，或心胸疼痛与阳虚症状共见\n",
    "- 阳虚证：畏寒肢冷、小便清长、面色晄白\n",
    "- 心悸怔忡，心胸憋闷或痛，气短，自汗，畏冷肢凉，神疲乏力，面色白，或面唇青紫，舌质淡胖或紫暗，苔白滑，脉弱或结或代\n",
    "\n",
    "### 心血瘀阻证\n",
    "\n",
    "- **心悸怔忡、心胸憋闷疼痛与瘀血症状共见为辨证的主要依据**\n",
    "- 血瘀证：疼痛、肿块、出血与肤色、舌色青紫等表现共现\n",
    "- 心悸怔忡，心胸憋闷疼痛，痛引肩背内臂，时作时止；或以刺痛为主，舌质晦暗或有青紫斑点，脉细、涩、结、代；或以心胸憋闷为主，体胖痰多，身重困倦，舌苔白腻，脉沉滑或沉涩；或以遇寒痛剧为主，得温痛减，畏寒肢冷，舌淡苔白，脉沉迟或沉紧；或以胀痛为主，与情志变化有关，喜太息，舌淡红，脉弦\n",
    "\n",
    "\n",
    "### 心火亢盛证\n",
    "\n",
    "- 心烦失眠、舌赤生疮、吐衄、尿赤与实热症状共见\n",
    "- 发热，口渴，心烦，失眠，便秘，尿黄，面红，舌尖红绛，苔黄，脉数有力。甚或口舌生疮、溃烂疼痛；或见小便短赤、灼热涩痛；或见吐血、衄血；或见狂躁谵语、神志不清\n",
    "\n",
    "\n",
    "### 痰蒙心神证\n",
    "\n",
    "- 神志抑郁、错乱、痴呆、昏迷与痰浊症状共见\n",
    "- 发热，口渴，胸闷，气粗，咯吐黄痰，喉间痰鸣，心烦，失眠，甚则神昏谵语，或狂躁妄动，打人毁物，不避亲疏，胡言乱语，哭笑无常，面赤，舌质红，苔黄腻，脉滑数\n",
    "\n",
    "\n",
    "### 瘀阻脑络证\n",
    "\n",
    "- **头痛、头晕与瘀血症状共见为辨证的主要依据**\n",
    "- 头晕、头痛经久不愈，痛如锥刺，痛处固定，或健忘，失眠，心悸，或头部外伤后昏不知人，面色晦暗，舌质紫暗或有斑点，脉细涩\n",
    "\n",
    "\n",
    "### 备注\n",
    "- 阳虚证多于气虚证共存\n",
    "### [参考来源](https://www.med66.com/zhongyineikezhuzhiyi/fudaoziliao/ha2111057194.shtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-09 13:57:06.870197\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "print(f'{datetime.today()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-09 13:57:06.885820\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "print(f'{datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据平衡使用的方法\n",
    "USE_randomDownSample = False\n",
    "USE_Tomek_links = False\n",
    "\n",
    "USE_ADASYN = False\n",
    "USE_randomOverSample = False\n",
    "USE_SMOTE = False\n",
    "\n",
    "USE_SMOTETomek = True\n",
    "\n",
    "# 特征选择\n",
    "USE_chi2 = False\n",
    "USE_f_classif = False\n",
    "USE_mutual_info_classif = False\n",
    "\n",
    "# LIME(Local Interpretable Model-Agnostic Explanations)\n",
    "USE_LIME = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>证名</th>\n",
       "      <th>病案号</th>\n",
       "      <th>性别</th>\n",
       "      <th>年龄</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "      <th>S4</th>\n",
       "      <th>S5</th>\n",
       "      <th>S6</th>\n",
       "      <th>...</th>\n",
       "      <th>S116</th>\n",
       "      <th>S117</th>\n",
       "      <th>S118</th>\n",
       "      <th>S119</th>\n",
       "      <th>S120</th>\n",
       "      <th>S121</th>\n",
       "      <th>S122</th>\n",
       "      <th>S123</th>\n",
       "      <th>S124</th>\n",
       "      <th>S125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>4.0</td>\n",
       "      <td>152769.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>3.0</td>\n",
       "      <td>130013.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>4.0</td>\n",
       "      <td>176082.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>696.00000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>146.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>788.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.010339</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.39977</td>\n",
       "      <td>0.084434</td>\n",
       "      <td>0.080414</td>\n",
       "      <td>0.110856</td>\n",
       "      <td>0.08386</td>\n",
       "      <td>0.008616</td>\n",
       "      <td>0.028719</td>\n",
       "      <td>0.149339</td>\n",
       "      <td>0.452613</td>\n",
       "      <td>0.057438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1743 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       证名       病案号   性别    年龄        S1        S2         S3        S4  \\\n",
       "0     4.0       NaN  1.0  31.0  0.000000  0.000000   0.000000  1.000000   \n",
       "1     1.0       NaN  1.0  93.0  0.000000  0.000000   0.000000  0.000000   \n",
       "2     1.0       NaN  1.0  10.0  0.000000  0.000000   0.000000  0.000000   \n",
       "3     3.0       NaN  1.0  10.0  0.000000  0.000000   0.000000  0.000000   \n",
       "4     1.0       NaN  1.0  37.0  0.000000  0.000000   0.000000  0.000000   \n",
       "...   ...       ...  ...   ...       ...       ...        ...       ...   \n",
       "1738  4.0  152769.0  1.0  71.0  0.000000  0.000000   0.000000  0.000000   \n",
       "1739  3.0  130013.0  2.0  70.0  0.000000  0.000000   0.000000  0.000000   \n",
       "1740  4.0  176082.0  1.0  73.0  0.000000  0.000000   0.000000  0.000000   \n",
       "1741  NaN       NaN  NaN   NaN  5.000000  9.000000  18.000000  8.000000   \n",
       "1742  NaN       NaN  NaN   NaN  0.002872  0.005169   0.010339  0.004595   \n",
       "\n",
       "            S5        S6  ...       S116        S117        S118        S119  \\\n",
       "0     0.000000  0.000000  ...    1.00000    0.000000    0.000000    0.000000   \n",
       "1     0.000000  0.000000  ...    0.00000    1.000000    1.000000    0.000000   \n",
       "2     0.000000  0.000000  ...    0.00000    1.000000    1.000000    0.000000   \n",
       "3     0.000000  0.000000  ...    0.00000    1.000000    1.000000    0.000000   \n",
       "4     0.000000  0.000000  ...    1.00000    0.000000    0.000000    0.000000   \n",
       "...        ...       ...  ...        ...         ...         ...         ...   \n",
       "1738  0.000000  0.000000  ...    0.00000    0.000000    0.000000    0.000000   \n",
       "1739  0.000000  0.000000  ...    0.00000    0.000000    0.000000    0.000000   \n",
       "1740  0.000000  0.000000  ...    1.00000    0.000000    0.000000    1.000000   \n",
       "1741  8.000000  4.000000  ...  696.00000  147.000000  140.000000  193.000000   \n",
       "1742  0.004595  0.002298  ...    0.39977    0.084434    0.080414    0.110856   \n",
       "\n",
       "           S120       S121       S122        S123        S124        S125  \n",
       "0       0.00000   0.000000   0.000000    0.000000    0.000000    0.000000  \n",
       "1       0.00000   0.000000   0.000000    0.000000    0.000000    0.000000  \n",
       "2       0.00000   0.000000   0.000000    0.000000    0.000000    0.000000  \n",
       "3       0.00000   0.000000   0.000000    0.000000    0.000000    0.000000  \n",
       "4       0.00000   0.000000   0.000000    0.000000    1.000000    0.000000  \n",
       "...         ...        ...        ...         ...         ...         ...  \n",
       "1738    0.00000   0.000000   0.000000    0.000000    1.000000    0.000000  \n",
       "1739    0.00000   0.000000   0.000000    1.000000    0.000000    0.000000  \n",
       "1740    0.00000   0.000000   0.000000    0.000000    1.000000    0.000000  \n",
       "1741  146.00000  15.000000  50.000000  260.000000  788.000000  100.000000  \n",
       "1742    0.08386   0.008616   0.028719    0.149339    0.452613    0.057438  \n",
       "\n",
       "[1743 rows x 129 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xinzongbiao = pd.read_excel('./input/心总表.xlsx', sheet_name='总表')\n",
    "xinzongbiao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nTSNE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m X \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39mfit_transform(X)\n\u001B[0;32m     10\u001B[0m tsne \u001B[38;5;241m=\u001B[39m TSNE(n_components\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, init\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpca\u001B[39m\u001B[38;5;124m'\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)\n\u001B[1;32m---> 11\u001B[0m X_tsne \u001B[38;5;241m=\u001B[39m \u001B[43mtsne\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m8\u001B[39m), dpi\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m9\u001B[39m):\n",
      "File \u001B[1;32mD:\\Software\\anaconda\\envs\\test\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1117\u001B[0m, in \u001B[0;36mTSNE.fit_transform\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1098\u001B[0m     \u001B[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \n\u001B[0;32m   1100\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1115\u001B[0m \u001B[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001B[39;00m\n\u001B[0;32m   1116\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1117\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1118\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_ \u001B[38;5;241m=\u001B[39m embedding\n\u001B[0;32m   1119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_\n",
      "File \u001B[1;32mD:\\Software\\anaconda\\envs\\test\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:838\u001B[0m, in \u001B[0;36mTSNE._fit\u001B[1;34m(self, X, skip_num_points)\u001B[0m\n\u001B[0;32m    836\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m must be a positive number or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    837\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmethod \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbarnes_hut\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 838\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    839\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    840\u001B[0m \u001B[43m        \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    841\u001B[0m \u001B[43m        \u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    842\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    844\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    845\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(\n\u001B[0;32m    846\u001B[0m         X, accept_sparse\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoo\u001B[39m\u001B[38;5;124m\"\u001B[39m], dtype\u001B[38;5;241m=\u001B[39m[np\u001B[38;5;241m.\u001B[39mfloat32, np\u001B[38;5;241m.\u001B[39mfloat64]\n\u001B[0;32m    847\u001B[0m     )\n",
      "File \u001B[1;32mD:\\Software\\anaconda\\envs\\test\\lib\\site-packages\\sklearn\\base.py:577\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    575\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 577\u001B[0m     X \u001B[38;5;241m=\u001B[39m check_array(X, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n\u001B[0;32m    578\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[1;32mD:\\Software\\anaconda\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py:899\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    893\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    894\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    895\u001B[0m             \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[0;32m    896\u001B[0m         )\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[1;32m--> 899\u001B[0m         \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    900\u001B[0m \u001B[43m            \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    901\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    902\u001B[0m \u001B[43m            \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    903\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    904\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    907\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n",
      "File \u001B[1;32mD:\\Software\\anaconda\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    124\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m allow_nan\n\u001B[0;32m    126\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m estimator_name\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    130\u001B[0m             \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[0;32m    131\u001B[0m             \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[0;32m    132\u001B[0m             msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    133\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    134\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    144\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    145\u001B[0m             )\n\u001B[1;32m--> 146\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n\u001B[0;32m    148\u001B[0m \u001B[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m X\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_nan:\n",
      "\u001B[1;31mValueError\u001B[0m: Input X contains NaN.\nTSNE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "zhenghou2id = readJSON('./input/zhenghou2id.json')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "scaler = StandardScaler()\n",
    "X = xinzongbiao.drop(columns='证名')\n",
    "y = xinzongbiao['证名']\n",
    "X = scaler.fit_transform(X)\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(8, 8), dpi=200)\n",
    "for i in range(1, 9):\n",
    "    plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set3(i), marker=i, label=zhenghou2id[str(i)])\n",
    "plt.legend()\n",
    "# plt.savefig('1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xinzongbiao.drop(columns=['病案号', '性别', '年龄'], inplace=True)\n",
    "xinzongbiao.dropna(inplace=True)\n",
    "xinzongbiao.astype(int)\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "xinzongbiao = xinzongbiao.rename(columns=id2feature)\n",
    "xinzongbiao['证名'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 心总表总共出现的症状频数\n",
    "xinzongbiao.drop(columns='证名').reset_index(drop=True).sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.心气虚症状统计 共621条\n",
    "c1 = xinzongbiao[xinzongbiao['证名'] == 1].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c1, np.round(c1 / xinzongbiao['证名'].value_counts()[1], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2.心血虚证 共70条\n",
    "c2 = xinzongbiao[xinzongbiao['证名'] == 2].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c2, np.round(c2 / xinzongbiao['证名'].value_counts()[2], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3.心阴虚证 共184条\n",
    "c3 = xinzongbiao[xinzongbiao['证名'] == 3].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c3, np.round(c3 / xinzongbiao['证名'].value_counts()[3], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 4.心阳虚证 共121条\n",
    "c4 = xinzongbiao[xinzongbiao['证名'] == 4].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c4, np.round(c4 / xinzongbiao['证名'].value_counts()[4], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 5.心血瘀阻证 共548条\n",
    "c5 = xinzongbiao[xinzongbiao['证名'] == 5].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c5, np.round(c5 / xinzongbiao['证名'].value_counts()[5], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 6.心火亢盛证 共53条\n",
    "c6 = xinzongbiao[xinzongbiao['证名'] == 6].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c6, np.round(c6 / xinzongbiao['证名'].value_counts()[6], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7.痰蒙心神证 共112条\n",
    "c7 = xinzongbiao[xinzongbiao['证名'] == 7].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c7, np.round(c7 / xinzongbiao['证名'].value_counts()[7], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 8.痰阻脑络证 32条\n",
    "c8 = xinzongbiao[xinzongbiao['证名'] == 8].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c8, np.round(c8 / xinzongbiao['证名'].value_counts()[8], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 心气血和心血瘀阻\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "logger.add('./log/{time}.log')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True,inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LSVC = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LSVC:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    import category_encoders as ce\n",
    "    # encoder = ce.TargetEncoder()\n",
    "    encoder = ce.CatBoostEncoder()\n",
    "    X_ce = encoder.fit_transform(X,y)\n",
    "    scaler = StandardScaler()\n",
    "    X_ce = scaler.fit_transform(X, y)\n",
    "    X_ce = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.009, penalty='l2', dual=False, random_state=64).fit(X_ce, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X_ce)\n",
    "    col = [c for c, i in zip(X_ce.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 621\n",
    "xxyz = data[data['证名'] == 4]  # 547\n",
    "# xqx = xqx.sample(180, random_state=64)\n",
    "# xxyz = xxyz.sample(120, random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(4, 4), dpi=200)\n",
    "for i in range(0, 2):\n",
    "    plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心气虚和心血瘀阻TSNE')\n",
    "plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "USE_randomDownSample, USE_Tomek_links, USE_ADASYN, USE_randomOverSample, USE_SMOTE, USE_SMOTETomek = False, False, False, False, False, True\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',\n",
    "        # 'n_estimators':trial.suggest_int('n_estimators',3500,3600),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", ['depthwise', 'lossguide']),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        'tree_method': 'exact',\n",
    "        # 'booster': 'dart',\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        # 'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param).fit(train_x, train_y)\n",
    "    preds = model.predict(valid_x)\n",
    "    return accuracy_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "# logger.info(study.best_value)\n",
    "logger.info(study.best_params)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=64)\n",
    "modelcv = xgb.XGBClassifier(**study.best_params)\n",
    "# modelcv.save_model(f'./output/model/{datetime.now()}.json')\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = cross_validate(modelcv, X_resampled, y_resampled, scoring=['accuracy', 'precision', 'recall', 'f1'], cv=kf)\n",
    "logger.warning(cv)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    # print(Counter(y_resampled.loc[train_index]),Counter(y_resampled.loc[test_index]))\n",
    "    model = xgb.XGBClassifier(**study.best_trial.params).fit(X_resampled.loc[train_index], y_resampled.loc[train_index])\n",
    "    preds = model.predict(X_resampled.loc[test_index])\n",
    "    accuracy = accuracy_score(y_resampled[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'accuracy\\t\\tmean:{round(np.mean(Accuracy), 3)}\\tstd:{round(np.std(Accuracy), 3)}')\n",
    "logger.warning(f'precision\\t\\tmean:{round(np.mean(Precision), 3)}\\tstd:{round(np.std(Precision), 3)}')\n",
    "logger.warning(f'recall\\t\\tmean:{round(np.mean(Recall), 3)}\\tstd:{round(np.std(Recall), 3)}')\n",
    "logger.warning(f'f1\\t\\tmean:{round(np.mean(F1), 3)}\\tstd:{round(np.std(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 心阴虚和心阳虚\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LASSO = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=50)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LASSO:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 2]  # 心阴虚184\n",
    "xxyz = data[data['证名'] == 3]  # 心阳虚121\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(4, 4), dpi=200)\n",
    "for i in range(0, 2):\n",
    "    plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心阴虚和心阳虚TSNE')\n",
    "plt.show()\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 14:40:47.352 | INFO     | __main__:<cell line: 89>:105 - 使用 SMOTETomek 混合采样\n",
      "2022-10-09 14:40:47.368 | INFO     | __main__:<cell line: 110>:110 - 采样前[(0, 184), (1, 121)],采样后[(0, 177), (1, 177)]\n",
      "2022-10-09 14:40:51.346 | WARNING  | __main__:<cell line: 143>:158 - 0.648\t0.632\t0.686\t0.658\n",
      "2022-10-09 14:40:51.377 | WARNING  | __main__:<cell line: 143>:158 - 0.746\t0.743\t0.743\t0.743\n",
      "2022-10-09 14:40:51.408 | WARNING  | __main__:<cell line: 143>:158 - 0.718\t0.674\t0.861\t0.756\n",
      "2022-10-09 14:40:51.424 | WARNING  | __main__:<cell line: 143>:158 - 0.507\t0.512\t0.583\t0.545\n",
      "2022-10-09 14:40:51.455 | WARNING  | __main__:<cell line: 143>:158 - 0.771\t0.788\t0.743\t0.765\n",
      "2022-10-09 14:40:51.455 | WARNING  | __main__:<cell line: 163>:163 - accuracy\t\tmean:0.678\tstd:0.095\n",
      "2022-10-09 14:40:51.455 | WARNING  | __main__:<cell line: 164>:164 - precision\t\tmean:0.67\tstd:0.095\n",
      "2022-10-09 14:40:51.455 | WARNING  | __main__:<cell line: 165>:165 - recall\t\tmean:0.723\tstd:0.09\n",
      "2022-10-09 14:40:51.455 | WARNING  | __main__:<cell line: 166>:166 - f1\t\tmean:0.693\tstd:0.083\n"
     ]
    }
   ],
   "source": [
    "# 心阴虚和心阳虚\n",
    "import collections\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]  #设置字体\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LASSO = False, False, False, False\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=40)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LASSO:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 2]  # 心阴虚184\n",
    "xxyz = data[data['证名'] == 3]  # 心阳虚121\n",
    "# xqx = xqx.sample(120, random_state=1024)\n",
    "# xxyz = xxyz.sample(100, random_state=1024)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "tmp = tmp.sample(frac=1)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "# tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# plt.figure(figsize=(4, 4), dpi=200)\n",
    "# for i in range(0, 2):\n",
    "#     plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "# plt.legend()\n",
    "# plt.title('心阴虚和心阳虚TSNE')\n",
    "# plt.show()\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'gbtree',\n",
    "        # 'enable_categorical':True,\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "        'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    }\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    return precision_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    dtrain = xgb.DMatrix(X_resampled.loc[train_index], label=y_resampled.loc[train_index])\n",
    "    dvalid = xgb.DMatrix(X_resampled.loc[test_index], label=y_resampled.loc[test_index])\n",
    "    model = xgb.train(study.best_trial.params, dtrain)\n",
    "    preds = model.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    accuracy = accuracy_score(y_resampled.loc[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled.loc[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled.loc[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled.loc[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    # explainer = shap.TreeExplainer(model)\n",
    "    # shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    # shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'accuracy\\t\\tmean:{round(np.mean(Accuracy), 3)}\\tstd:{round(np.std(Accuracy), 3)}')\n",
    "logger.warning(f'precision\\t\\tmean:{round(np.mean(Precision), 3)}\\tstd:{round(np.std(Precision), 3)}')\n",
    "logger.warning(f'recall\\t\\tmean:{round(np.mean(Recall), 3)}\\tstd:{round(np.std(Recall), 3)}')\n",
    "logger.warning(f'f1\\t\\tmean:{round(np.mean(F1), 3)}\\tstd:{round(np.std(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 心阴虚和心血瘀阻\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "else:\n",
    "    logger.info('不进行特征选择')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 2]  # 心阴虚184\n",
    "xxyz = data[data['证名'] == 4]  # 心血瘀阻547\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'gbtree',\n",
    "        # 'enable_categorical':True,\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "        'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    }\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    return precision_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    dtrain = xgb.DMatrix(X_resampled.loc[train_index], label=y_resampled.loc[train_index])\n",
    "    dvalid = xgb.DMatrix(X_resampled.loc[test_index], label=y_resampled.loc[test_index])\n",
    "    model = xgb.train(study.best_trial.params, dtrain)\n",
    "    preds = model.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    accuracy = accuracy_score(y_resampled.loc[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled.loc[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled.loc[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled.loc[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'均值：\\t{round(np.mean(Accuracy), 3)}\\t{round(np.mean(Precision), 3)}\\t'\n",
    "               f'{round(np.mean(Recall), 3)}\\t{round(np.mean(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 心气虚和心阴虚\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LASSO = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LASSO:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 心气血621\n",
    "xxyz = data[data['证名'] == 2]  # 心阴虚184\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "xqx = xqx.sample(150, random_state=1024)\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "# tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# plt.figure(figsize=(4, 4), dpi=200)\n",
    "# for i in range(0, 2):\n",
    "#     plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "# plt.legend()\n",
    "# plt.title('心气虚和心阴虚TSNE')\n",
    "# plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X_resampled)\n",
    "plt.figure(figsize=(4, 4), dpi=200)\n",
    "for i in range(0, 2):\n",
    "    plt.scatter(X_tsne[y_resampled == i][:, 0], X_tsne[y_resampled == i][:, 1], cmap=plt.cm.Set1(i), marker=i,\n",
    "                label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心气虚和心阴虚TSNE')\n",
    "plt.show()\n",
    "\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'gbtree',\n",
    "        # 'enable_categorical':True,\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "        'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    }\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    return precision_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    dtrain = xgb.DMatrix(X_resampled.loc[train_index], label=y_resampled.loc[train_index])\n",
    "    dvalid = xgb.DMatrix(X_resampled.loc[test_index], label=y_resampled.loc[test_index])\n",
    "    model = xgb.train(study.best_trial.params, dtrain)\n",
    "    preds = model.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    accuracy = accuracy_score(y_resampled.loc[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled.loc[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled.loc[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled.loc[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'均值：\\taccuracy:{round(np.mean(Accuracy), 3)}\\tprecision:{round(np.mean(Precision), 3)}\\t'\n",
    "               f'recall:{round(np.mean(Recall), 3)}\\tf1:{round(np.mean(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 心气虚和心阳虚\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "else:\n",
    "    logger.info('不进行特征选择')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 3]  # 心阳虚121\n",
    "xxyz = data[data['证名'] == 0]  # 心气血621\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'gbtree',\n",
    "        # 'enable_categorical':True,\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "        'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    }\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    return precision_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    dtrain = xgb.DMatrix(X_resampled.loc[train_index], label=y_resampled.loc[train_index])\n",
    "    dvalid = xgb.DMatrix(X_resampled.loc[test_index], label=y_resampled.loc[test_index])\n",
    "    model = xgb.train(study.best_trial.params, dtrain)\n",
    "    preds = model.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    accuracy = accuracy_score(y_resampled.loc[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled.loc[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled.loc[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled.loc[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'均值：\\t{round(np.mean(Accuracy), 3)}\\t{round(np.mean(Precision), 3)}\\t'\n",
    "               f'{round(np.mean(Recall), 3)}\\t{round(np.mean(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LASSO = False, False, False, True\n",
    "if USE_chi2:\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "    logger.info(f'使用 卡方检验 进行特诊筛选,剩余{X.shape[1]}个特征')\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "    logger.info(f'使用 卡方检验 进行特诊筛选,剩余{X.shape[1]}个特征')\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "    logger.info(f'使用 卡方检验 进行特诊筛选,剩余{X.shape[1]}个特征')\n",
    "elif USE_LASSO:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "# Accuracy = []\n",
    "# Precision = []\n",
    "# Recall = []\n",
    "# F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 621\n",
    "xxyz = data[data['证名'] == 4]  # 547\n",
    "# xqx= xqx.sample(180,random_state=64)\n",
    "# xxyz = xxyz.sample(180,random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "\n",
    "# tsne = TSNE(n_components=2,init='pca',random_state=64)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# plt.figure(figsize=(4,4),dpi=200)\n",
    "# for i in range(0,2):\n",
    "#     plt.scatter(X_tsne[y==i][:,0],X_tsne[y==i][:,1],cmap=plt.cm.Set1(i),marker=i,label=str(i))\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# if USE_randomDownSample:\n",
    "#     logger.info('使用 randomDownSample 下采样')\n",
    "#     sampler = RandomUnderSampler(random_state=64)\n",
    "# elif USE_Tomek_links:\n",
    "#     logger.info('使用 TomekLinks 下采样')\n",
    "#     sampler = TomekLinks()\n",
    "# elif USE_ADASYN:\n",
    "#     logger.info('使用 ADASYN 上采样')\n",
    "#     sampler = ADASYN(random_state=64)\n",
    "# elif USE_randomOverSample:\n",
    "#     logger.info('使用 randomOverSample 上采样')\n",
    "#     sampler = RandomOverSampler(random_state=64)\n",
    "# elif USE_SMOTE:\n",
    "#     logger.info('使用 SMOTE 上采样')\n",
    "#     sampler = SMOTE(random_state=64)\n",
    "# elif USE_SMOTETomek:\n",
    "#     logger.info('使用 SMOTETomek 混合采样')\n",
    "#     sampler = SMOTETomek(random_state=64)\n",
    "# else:\n",
    "#     assert False, '没有平衡数据'\n",
    "# X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "# logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "def objective(trial):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=1024)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    param = {\n",
    "        'silent': True,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "    gbm = lgb.LGBMClassifier(**param)\n",
    "    gbm.fit(train_x, train_y)\n",
    "    preds = gbm.predict(test_x)\n",
    "    # print(Counter(preds))\n",
    "    # pred_labels = np.rint(preds)\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_y, preds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "print(study.best_value)\n",
    "# kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=64)\n",
    "# modelcv = lgb.LGBMClassifier(**study.best_params)\n",
    "# # modelcv.save_model(f'./output/model/{datetime.now()}.json')\n",
    "# from sklearn.model_selection import cross_validate\n",
    "# cv = cross_validate(modelcv,X,y,scoring=['accuracy','precision','recall','f1'],cv=kf)\n",
    "# pprint(cv)\n",
    "# modelkf = lgb.LGBMClassifier(**study.best_trial.params)\n",
    "# for train_index, test_index in kf.split(X, y):\n",
    "#     # print(Counter(y_resampled.loc[train_index]),Counter(y_resampled.loc[test_index]))\n",
    "#     model =modelkf.fit(X.loc[train_index],y.loc[train_index])\n",
    "#     preds = model.predict(X.loc[test_index])\n",
    "#     accuracy = accuracy_score(y[test_index], preds)\n",
    "#     Accuracy.append(accuracy)\n",
    "#     precision = precision_score(y[test_index], preds)\n",
    "#     Precision.append(precision)\n",
    "#     recall = recall_score(y[test_index], preds)\n",
    "#     Recall.append(recall)\n",
    "#     f1 = f1_score(y[test_index], preds)\n",
    "#     F1.append(f1)\n",
    "#     logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "#                    f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "#     explainer = shap.TreeExplainer(model)\n",
    "#     shap_values_XGBoost_train = explainer.shap_values(X.loc[train_index])\n",
    "#     shap.summary_plot(shap_values_XGBoost_train, X.iloc[train_index])\n",
    "# logger.warning(f'accuracy\\t\\tmean:{round(np.mean(Accuracy), 3)}\\tstd:{round(np.std(Accuracy), 3)}')\n",
    "# logger.warning(f'precision\\t\\tmean:{round(np.mean(Precision), 3)}\\tstd:{round(np.std(Precision), 3)}')\n",
    "# logger.warning(f'recall\\t\\tmean:{round(np.mean(Recall), 3)}\\tstd:{round(np.std(Recall), 3)}')\n",
    "# logger.warning(f'f1\\t\\tmean:{round(np.mean(F1), 3)}\\tstd:{round(np.std(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "logger.add('./log/{time}.log')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LSVC = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LSVC:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LSVC 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 621\n",
    "xxyz = data[data['证名'] == 4]  # 547\n",
    "xqx = xqx.sample(300, random_state=64)\n",
    "xxyz = xxyz.sample(300, random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "# tsne = TSNE(n_components=2,init='pca',random_state=64)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# plt.figure(figsize=(4,4),dpi=200)\n",
    "# for i in range(0,2):\n",
    "#     plt.scatter(X_tsne[y==i][:,0],X_tsne[y==i][:,1],cmap=plt.cm.Set1(i),marker=i,label=str(i))\n",
    "# plt.legend()\n",
    "# plt.title('心气虚和心血瘀阻TSNE')\n",
    "# plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "USE_randomDownSample, USE_Tomek_links, USE_ADASYN, USE_randomOverSample, USE_SMOTE, USE_SMOTETomek = False, False, False, False, False, True\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25)\n",
    "    param = {\n",
    "        'hidden_layer_sizes': (100, 100, 100),\n",
    "        'activation': trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "        'alpha': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'random_state': 1024\n",
    "    }\n",
    "    clf = MLPClassifier(**param)\n",
    "    clf.fit(train_x, train_y)\n",
    "    preds = clf.predict(test_x)\n",
    "    return accuracy_score(test_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=400)\n",
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "logger.add('./log/{time}.log')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LSVC = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LSVC:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LSVC 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 621\n",
    "xxyz = data[data['证名'] == 4]  # 547\n",
    "# xqx= xqx.sample(180,random_state=64)\n",
    "# xxyz = xxyz.sample(120,random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "# tsne = TSNE(n_components=2,init='pca',random_state=64)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# plt.figure(figsize=(4,4),dpi=200)\n",
    "# for i in range(0,2):\n",
    "#     plt.scatter(X_tsne[y==i][:,0],X_tsne[y==i][:,1],cmap=plt.cm.Set1(i),marker=i,label=str(i))\n",
    "# plt.legend()\n",
    "# plt.title('心气虚和心血瘀阻TSNE')\n",
    "# plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "USE_randomDownSample, USE_Tomek_links, USE_ADASYN, USE_randomOverSample, USE_SMOTE, USE_SMOTETomek = False, False, False, False, False, True\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=1024)\n",
    "    param = {\n",
    "        'C': trial.suggest_float('C', 0, 1),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    }\n",
    "    model = SVC(**param)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(test_X)\n",
    "    return accuracy_score(test_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "n_epoch = 50\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "def preprocess(path, sheet_name):\n",
    "    \"\"\"\n",
    "    对心总病数据进行预处理\n",
    "    :param path: 心总病文件路径\n",
    "    :param sheet_name: sheet的名称\n",
    "    :return: X,y\n",
    "    \"\"\"\n",
    "\n",
    "    xzb = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    xzb.drop(columns=['病案号'], inplace=True)\n",
    "    xzb.drop(labels=[1742, 1741], axis=0, inplace=True)\n",
    "    xzb.drop(labels=xzb[xzb['性别'].isna()].index, inplace=True)\n",
    "    xzb = xzb.sample(frac=1).astype(int)\n",
    "    y = xzb['证名']\n",
    "    X = xzb.drop(labels=['证名', '性别', '年龄'], axis=1)\n",
    "\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "    xqx = data[data['证名'] == 0]  # 621\n",
    "    xxyz = data[data['证名'] == 4]  # 547\n",
    "    xqx['证名'] = 0\n",
    "    xxyz['证名'] = 1\n",
    "    tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "    X = tmp.drop(columns='证名')\n",
    "    y = tmp['证名']\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "class XZB(Dataset):\n",
    "    def __init__(self, X, y, train=True):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1024)\n",
    "        if train:\n",
    "            X_train.reset_index(drop=True, inplace=True)\n",
    "            y_train.reset_index(drop=True, inplace=True)\n",
    "            self.data, self.labels = X_train, y_train\n",
    "        else:\n",
    "            X_test.reset_index(drop=True, inplace=True)\n",
    "            y_test.reset_index(drop=True, inplace=True)\n",
    "            self.data, self.labels = X_test, y_test\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(len(self.labels),self.labels)\n",
    "        return len(self.labels) - 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data.loc[index].values, self.labels.loc[index]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(73, 40)\n",
    "        self.fc2 = nn.Linear(40, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = F.relu(self.fc1(input.float()))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        return F.softmax(self.fc3(out), dim=1)\n",
    "\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "train_dataset = XZB(X, y, train=True)\n",
    "test_dataset = XZB(X, y, train=False)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "lossfunc = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01)\n",
    "for epoch in range(n_epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, target in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = lossfunc(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    train_loss = train_loss / len(train_dataloader.dataset)\n",
    "    print('Epoch:  {}  \\tTraining Loss: {:.6f}'.format(epoch + 1, train_loss))\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_dataloader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "            100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 心阳虚和痰蒙心神证\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "logger.add('./log/{time}.log')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LSVC = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LSVC:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 3]  # 心阳虚\n",
    "xxyz = data[data['证名'] == 6]  # 心血瘀阻\n",
    "# xqx = xqx.sample(180, random_state=64)\n",
    "# xxyz = xxyz.sample(120, random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(4, 4), dpi=200)\n",
    "for i in range(0, 2):\n",
    "    plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心阳虚和痰蒙心神TSNE')\n",
    "plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "USE_randomDownSample, USE_Tomek_links, USE_ADASYN, USE_randomOverSample, USE_SMOTE, USE_SMOTETomek = False, False, False, False, False, True\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',\n",
    "        # 'n_estimators':trial.suggest_int('n_estimators',3500,3600),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", ['depthwise', 'lossguide']),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'dart',\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        # 'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param).fit(train_x, train_y)\n",
    "    preds = model.predict(valid_x)\n",
    "    return accuracy_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "# logger.info(study.best_value)\n",
    "logger.info(study.best_params)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=64)\n",
    "modelcv = xgb.XGBClassifier(**study.best_params)\n",
    "# modelcv.save_model(f'./output/model/{datetime.now()}.json')\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = cross_validate(modelcv, X_resampled, y_resampled, scoring=['accuracy', 'precision', 'recall', 'f1'], cv=kf)\n",
    "logger.warning(cv)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    # print(Counter(y_resampled.loc[train_index]),Counter(y_resampled.loc[test_index]))\n",
    "    model = xgb.XGBClassifier(**study.best_trial.params).fit(X_resampled.loc[train_index], y_resampled.loc[train_index])\n",
    "    preds = model.predict(X_resampled.loc[test_index])\n",
    "    accuracy = accuracy_score(y_resampled[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'accuracy\\t\\tmean:{round(np.mean(Accuracy), 3)}\\tstd:{round(np.std(Accuracy), 3)}')\n",
    "logger.warning(f'precision\\t\\tmean:{round(np.mean(Precision), 3)}\\tstd:{round(np.std(Precision), 3)}')\n",
    "logger.warning(f'recall\\t\\tmean:{round(np.mean(Recall), 3)}\\tstd:{round(np.std(Recall), 3)}')\n",
    "logger.warning(f'f1\\t\\tmean:{round(np.mean(F1), 3)}\\tstd:{round(np.std(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "heart = pd.read_csv('./input/heart/heart.csv')\n",
    "X = heart.drop(columns='target')\n",
    "y = heart['target']\n",
    "print(Counter(y))\n",
    "# TSNE\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "tsne = TSNE(n_components=2,init='pca',random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(4,4),dpi=200)\n",
    "for i in range(0,2):\n",
    "    plt.scatter(X_tsne[y==i][:,0],X_tsne[y==i][:,1],cmap=plt.cm.Set1(i),marker=i,label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心脏病')\n",
    "plt.show()\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.1, random_state=64)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',\n",
    "        # 'n_estimators':trial.suggest_int('n_estimators',3500,3600),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", ['depthwise', 'lossguide']),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'dart',\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        # 'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param).fit(train_x, train_y)\n",
    "    preds = model.predict(valid_x)\n",
    "    return accuracy_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "print(study.best_value)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=64)\n",
    "modelcv = xgb.XGBClassifier(**study.best_params)\n",
    "# modelcv.save_model(f'./output/model/{datetime.now()}.json')\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = cross_validate(modelcv, X, y, scoring=['accuracy', 'precision', 'recall', 'f1'], cv=kf)\n",
    "print(cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "397224b3cdb3a23561542feed94c5cf45952f2aadda9fa59ec57fed9311c625f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
