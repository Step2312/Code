{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 心病辨证要点\n",
    "\n",
    "### 心气虚证：\n",
    "- 心悸怔忡与气虚症状共见\n",
    "- 气虚证：神疲乏力、少气懒言、脉虚、动则诸症加剧为主要表现\n",
    "- 心悸，胸闷，气短，精神疲倦，或有自汗，面色淡白，舌质淡，脉虚\n",
    "\n",
    "### 心血虚证：\n",
    "- 心悸、失眠、多梦与血虚症状共见\n",
    "- 血虚证：面、睑、唇、舌色淡白，脉细等为主要表现\n",
    "- 心悸、头晕眼花、失眠、多梦、健忘、面色淡白或萎黄、舌色淡、脉细无力\n",
    "\n",
    "### 心阴虚证\n",
    "- 心悸、心烦、失眠与虚热症状共现\n",
    "- 阴虚证：口咽干燥、五心烦躁、潮热盗汗、两颧潮红、舌红少苔、脉细数\n",
    "- 心烦、心悸，失眠，多梦，口燥咽干，形体消瘦，或见手足心热，潮热盗汗，两颧潮红，舌红少苔乏津，脉细数\n",
    "\n",
    "\n",
    "### 心阳虚证\n",
    "\n",
    "- 心悸怔忡，或心胸疼痛与阳虚症状共见\n",
    "- 阳虚证：畏寒肢冷、小便清长、面色晄白\n",
    "- 心悸怔忡，心胸憋闷或痛，气短，自汗，畏冷肢凉，神疲乏力，面色白，或面唇青紫，舌质淡胖或紫暗，苔白滑，脉弱或结或代\n",
    "\n",
    "### 心血瘀阻证\n",
    "\n",
    "- **心悸怔忡、心胸憋闷疼痛与瘀血症状共见为辨证的主要依据**\n",
    "- 血瘀证：疼痛、肿块、出血与肤色、舌色青紫等表现共现\n",
    "- 心悸怔忡，心胸憋闷疼痛，痛引肩背内臂，时作时止；或以刺痛为主，舌质晦暗或有青紫斑点，脉细、涩、结、代；或以心胸憋闷为主，体胖痰多，身重困倦，舌苔白腻，脉沉滑或沉涩；或以遇寒痛剧为主，得温痛减，畏寒肢冷，舌淡苔白，脉沉迟或沉紧；或以胀痛为主，与情志变化有关，喜太息，舌淡红，脉弦\n",
    "\n",
    "\n",
    "### 心火亢盛证\n",
    "\n",
    "- 心烦失眠、舌赤生疮、吐衄、尿赤与实热症状共见\n",
    "- 发热，口渴，心烦，失眠，便秘，尿黄，面红，舌尖红绛，苔黄，脉数有力。甚或口舌生疮、溃烂疼痛；或见小便短赤、灼热涩痛；或见吐血、衄血；或见狂躁谵语、神志不清\n",
    "\n",
    "\n",
    "### 痰蒙心神证\n",
    "\n",
    "- 神志抑郁、错乱、痴呆、昏迷与痰浊症状共见\n",
    "- 发热，口渴，胸闷，气粗，咯吐黄痰，喉间痰鸣，心烦，失眠，甚则神昏谵语，或狂躁妄动，打人毁物，不避亲疏，胡言乱语，哭笑无常，面赤，舌质红，苔黄腻，脉滑数\n",
    "\n",
    "\n",
    "### 瘀阻脑络证\n",
    "\n",
    "- **头痛、头晕与瘀血症状共见为辨证的主要依据**\n",
    "- 头晕、头痛经久不愈，痛如锥刺，痛处固定，或健忘，失眠，心悸，或头部外伤后昏不知人，面色晦暗，舌质紫暗或有斑点，脉细涩\n",
    "\n",
    "\n",
    "### 备注\n",
    "- 阳虚证多于气虚证共存\n",
    "### [参考来源](https://www.med66.com/zhongyineikezhuzhiyi/fudaoziliao/ha2111057194.shtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "print(f'{datetime.today()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "print(f'{datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 数据平衡使用的方法\n",
    "USE_randomDownSample = False\n",
    "USE_Tomek_links = False\n",
    "\n",
    "USE_ADASYN = False\n",
    "USE_randomOverSample = False\n",
    "USE_SMOTE = False\n",
    "\n",
    "USE_SMOTETomek = True\n",
    "\n",
    "# 特征选择\n",
    "USE_chi2 = False\n",
    "USE_f_classif = False\n",
    "USE_mutual_info_classif = False\n",
    "\n",
    "# LIME(Local Interpretable Model-Agnostic Explanations)\n",
    "USE_LIME = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xinzongbiao = pd.read_excel('./input/心总表.xlsx', sheet_name='总表')\n",
    "xinzongbiao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "zhenghou2id = readJSON('./input/zhenghou2id.json')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "scaler = StandardScaler()\n",
    "X = xinzongbiao.drop(columns='证名')\n",
    "y = xinzongbiao['证名']\n",
    "X = scaler.fit_transform(X)\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(8, 8), dpi=200)\n",
    "for i in range(1, 9):\n",
    "    plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set3(i), marker=i, label=zhenghou2id[str(i)])\n",
    "plt.legend()\n",
    "# plt.savefig('1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xinzongbiao.drop(columns=['病案号', '性别', '年龄'], inplace=True)\n",
    "xinzongbiao.dropna(inplace=True)\n",
    "xinzongbiao.astype(int)\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "xinzongbiao = xinzongbiao.rename(columns=id2feature)\n",
    "xinzongbiao['证名'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 心总表总共出现的症状频数\n",
    "xinzongbiao.drop(columns='证名').reset_index(drop=True).sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1.心气虚症状统计 共621条\n",
    "c1 = xinzongbiao[xinzongbiao['证名'] == 1].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c1, np.round(c1 / xinzongbiao['证名'].value_counts()[1], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 2.心血虚证 共70条\n",
    "c2 = xinzongbiao[xinzongbiao['证名'] == 2].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c2, np.round(c2 / xinzongbiao['证名'].value_counts()[2], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 3.心阴虚证 共184条\n",
    "c3 = xinzongbiao[xinzongbiao['证名'] == 3].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c3, np.round(c3 / xinzongbiao['证名'].value_counts()[3], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4.心阳虚证 共121条\n",
    "c4 = xinzongbiao[xinzongbiao['证名'] == 4].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c4, np.round(c4 / xinzongbiao['证名'].value_counts()[4], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 5.心血瘀阻证 共548条\n",
    "c5 = xinzongbiao[xinzongbiao['证名'] == 5].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c5, np.round(c5 / xinzongbiao['证名'].value_counts()[5], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 6.心火亢盛证 共53条\n",
    "c6 = xinzongbiao[xinzongbiao['证名'] == 6].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c6, np.round(c6 / xinzongbiao['证名'].value_counts()[6], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 7.痰蒙心神证 共112条\n",
    "c7 = xinzongbiao[xinzongbiao['证名'] == 7].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c7, np.round(c7 / xinzongbiao['证名'].value_counts()[7], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 8.痰阻脑络证 32条\n",
    "c8 = xinzongbiao[xinzongbiao['证名'] == 8].drop(columns='证名').reset_index(drop=True).sum().sort_values(\n",
    "    ascending=False)\n",
    "pd.DataFrame(data=[c8, np.round(c8 / xinzongbiao['证名'].value_counts()[8], 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 心气血和心血瘀阻\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "logger.add('./log/{time}.log')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "y.reset_index(drop=True,inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LSVC = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LSVC:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    import category_encoders as ce\n",
    "    # encoder = ce.TargetEncoder()\n",
    "    encoder = ce.CatBoostEncoder()\n",
    "    X_ce = encoder.fit_transform(X,y)\n",
    "    scaler = StandardScaler()\n",
    "    X_ce = scaler.fit_transform(X, y)\n",
    "    X_ce = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.009, penalty='l2', dual=False, random_state=64).fit(X_ce, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X_ce)\n",
    "    col = [c for c, i in zip(X_ce.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 621\n",
    "xxyz = data[data['证名'] == 4]  # 547\n",
    "# xqx = xqx.sample(180, random_state=64)\n",
    "# xxyz = xxyz.sample(120, random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(4, 4), dpi=200)\n",
    "for i in range(0, 2):\n",
    "    plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心气虚和心血瘀阻TSNE')\n",
    "plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "USE_randomDownSample, USE_Tomek_links, USE_ADASYN, USE_randomOverSample, USE_SMOTE, USE_SMOTETomek = False, False, False, False, False, True\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',\n",
    "        # 'n_estimators':trial.suggest_int('n_estimators',3500,3600),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", ['depthwise', 'lossguide']),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        'tree_method': 'exact',\n",
    "        # 'booster': 'dart',\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        # 'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param).fit(train_x, train_y)\n",
    "    preds = model.predict(valid_x)\n",
    "    return accuracy_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "# logger.info(study.best_value)\n",
    "logger.info(study.best_params)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=64)\n",
    "modelcv = xgb.XGBClassifier(**study.best_params)\n",
    "# modelcv.save_model(f'./output/model/{datetime.now()}.json')\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = cross_validate(modelcv, X_resampled, y_resampled, scoring=['accuracy', 'precision', 'recall', 'f1'], cv=kf)\n",
    "logger.warning(cv)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    # print(Counter(y_resampled.loc[train_index]),Counter(y_resampled.loc[test_index]))\n",
    "    model = xgb.XGBClassifier(**study.best_trial.params).fit(X_resampled.loc[train_index], y_resampled.loc[train_index])\n",
    "    preds = model.predict(X_resampled.loc[test_index])\n",
    "    accuracy = accuracy_score(y_resampled[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'accuracy\\t\\tmean:{round(np.mean(Accuracy), 3)}\\tstd:{round(np.std(Accuracy), 3)}')\n",
    "logger.warning(f'precision\\t\\tmean:{round(np.mean(Precision), 3)}\\tstd:{round(np.std(Precision), 3)}')\n",
    "logger.warning(f'recall\\t\\tmean:{round(np.mean(Recall), 3)}\\tstd:{round(np.std(Recall), 3)}')\n",
    "logger.warning(f'f1\\t\\tmean:{round(np.mean(F1), 3)}\\tstd:{round(np.std(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 心阴虚和心阳虚\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LASSO = True, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=50)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LASSO:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 2]  # 心阴虚184\n",
    "xxyz = data[data['证名'] == 3]  # 心阳虚121\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(4, 4), dpi=200)\n",
    "for i in range(0, 2):\n",
    "    plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心阴虚和心阳虚TSNE')\n",
    "plt.show()\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 心阴虚和心阳虚\n",
    "import collections\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "plt.rcParams[\"font.sans-serif\"] = [\"SimHei\"]  #设置字体\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LASSO = True, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=40)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LASSO:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 2]  # 心阴虚184\n",
    "xxyz = data[data['证名'] == 3]  # 心阳虚121\n",
    "xqx = xqx.sample(120, random_state=1024)\n",
    "xxyz = xxyz.sample(100, random_state=1024)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "tmp = tmp.sample(frac=1)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(4, 4), dpi=200)\n",
    "for i in range(0, 2):\n",
    "    plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心阴虚和心阳虚TSNE')\n",
    "plt.show()\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'gbtree',\n",
    "        # 'enable_categorical':True,\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "        'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    }\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    return precision_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    dtrain = xgb.DMatrix(X_resampled.loc[train_index], label=y_resampled.loc[train_index])\n",
    "    dvalid = xgb.DMatrix(X_resampled.loc[test_index], label=y_resampled.loc[test_index])\n",
    "    model = xgb.train(study.best_trial.params, dtrain)\n",
    "    preds = model.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    accuracy = accuracy_score(y_resampled.loc[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled.loc[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled.loc[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled.loc[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'accuracy\\t\\tmean:{round(np.mean(Accuracy), 3)}\\tstd:{round(np.std(Accuracy), 3)}')\n",
    "logger.warning(f'precision\\t\\tmean:{round(np.mean(Precision), 3)}\\tstd:{round(np.std(Precision), 3)}')\n",
    "logger.warning(f'recall\\t\\tmean:{round(np.mean(Recall), 3)}\\tstd:{round(np.std(Recall), 3)}')\n",
    "logger.warning(f'f1\\t\\tmean:{round(np.mean(F1), 3)}\\tstd:{round(np.std(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 心阴虚和心血瘀阻\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "else:\n",
    "    logger.info('不进行特征选择')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 2]  # 心阴虚184\n",
    "xxyz = data[data['证名'] == 4]  # 心血瘀阻547\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'gbtree',\n",
    "        # 'enable_categorical':True,\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "        'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    }\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    return precision_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    dtrain = xgb.DMatrix(X_resampled.loc[train_index], label=y_resampled.loc[train_index])\n",
    "    dvalid = xgb.DMatrix(X_resampled.loc[test_index], label=y_resampled.loc[test_index])\n",
    "    model = xgb.train(study.best_trial.params, dtrain)\n",
    "    preds = model.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    accuracy = accuracy_score(y_resampled.loc[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled.loc[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled.loc[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled.loc[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'均值：\\t{round(np.mean(Accuracy), 3)}\\t{round(np.mean(Precision), 3)}\\t'\n",
    "               f'{round(np.mean(Recall), 3)}\\t{round(np.mean(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 心气虚和心阴虚\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LASSO = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LASSO:\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 心气血621\n",
    "xxyz = data[data['证名'] == 2]  # 心阴虚184\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "xqx = xqx.sample(150, random_state=1024)\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "# tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# plt.figure(figsize=(4, 4), dpi=200)\n",
    "# for i in range(0, 2):\n",
    "#     plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "# plt.legend()\n",
    "# plt.title('心气虚和心阴虚TSNE')\n",
    "# plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "\n",
    "X_tsne = tsne.fit_transform(X_resampled)\n",
    "plt.figure(figsize=(4, 4), dpi=200)\n",
    "for i in range(0, 2):\n",
    "    plt.scatter(X_tsne[y_resampled == i][:, 0], X_tsne[y_resampled == i][:, 1], cmap=plt.cm.Set1(i), marker=i,\n",
    "                label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心气虚和心阴虚TSNE')\n",
    "plt.show()\n",
    "\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'gbtree',\n",
    "        # 'enable_categorical':True,\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "        'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    }\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    return precision_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    dtrain = xgb.DMatrix(X_resampled.loc[train_index], label=y_resampled.loc[train_index])\n",
    "    dvalid = xgb.DMatrix(X_resampled.loc[test_index], label=y_resampled.loc[test_index])\n",
    "    model = xgb.train(study.best_trial.params, dtrain)\n",
    "    preds = model.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    accuracy = accuracy_score(y_resampled.loc[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled.loc[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled.loc[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled.loc[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'均值：\\taccuracy:{round(np.mean(Accuracy), 3)}\\tprecision:{round(np.mean(Precision), 3)}\\t'\n",
    "               f'recall:{round(np.mean(Recall), 3)}\\tf1:{round(np.mean(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 心气虚和心阳虚\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "else:\n",
    "    logger.info('不进行特征选择')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 3]  # 心阳虚121\n",
    "xxyz = data[data['证名'] == 0]  # 心气血621\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'gbtree',\n",
    "        # 'enable_categorical':True,\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 2, 10),\n",
    "        'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    }\n",
    "    bst = xgb.train(param, dtrain)\n",
    "    preds = bst.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    return precision_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    dtrain = xgb.DMatrix(X_resampled.loc[train_index], label=y_resampled.loc[train_index])\n",
    "    dvalid = xgb.DMatrix(X_resampled.loc[test_index], label=y_resampled.loc[test_index])\n",
    "    model = xgb.train(study.best_trial.params, dtrain)\n",
    "    preds = model.predict(dvalid)\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    accuracy = accuracy_score(y_resampled.loc[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled.loc[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled.loc[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled.loc[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'均值：\\t{round(np.mean(Accuracy), 3)}\\t{round(np.mean(Precision), 3)}\\t'\n",
    "               f'{round(np.mean(Recall), 3)}\\t{round(np.mean(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import sklearn\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LASSO = False, False, False, True\n",
    "if USE_chi2:\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "    logger.info(f'使用 卡方检验 进行特诊筛选,剩余{X.shape[1]}个特征')\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "    logger.info(f'使用 卡方检验 进行特诊筛选,剩余{X.shape[1]}个特征')\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "    logger.info(f'使用 卡方检验 进行特诊筛选,剩余{X.shape[1]}个特征')\n",
    "elif USE_LASSO:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "# Accuracy = []\n",
    "# Precision = []\n",
    "# Recall = []\n",
    "# F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 621\n",
    "xxyz = data[data['证名'] == 4]  # 547\n",
    "# xqx= xqx.sample(180,random_state=64)\n",
    "# xxyz = xxyz.sample(180,random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "\n",
    "# tsne = TSNE(n_components=2,init='pca',random_state=64)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# plt.figure(figsize=(4,4),dpi=200)\n",
    "# for i in range(0,2):\n",
    "#     plt.scatter(X_tsne[y==i][:,0],X_tsne[y==i][:,1],cmap=plt.cm.Set1(i),marker=i,label=str(i))\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# if USE_randomDownSample:\n",
    "#     logger.info('使用 randomDownSample 下采样')\n",
    "#     sampler = RandomUnderSampler(random_state=64)\n",
    "# elif USE_Tomek_links:\n",
    "#     logger.info('使用 TomekLinks 下采样')\n",
    "#     sampler = TomekLinks()\n",
    "# elif USE_ADASYN:\n",
    "#     logger.info('使用 ADASYN 上采样')\n",
    "#     sampler = ADASYN(random_state=64)\n",
    "# elif USE_randomOverSample:\n",
    "#     logger.info('使用 randomOverSample 上采样')\n",
    "#     sampler = RandomOverSampler(random_state=64)\n",
    "# elif USE_SMOTE:\n",
    "#     logger.info('使用 SMOTE 上采样')\n",
    "#     sampler = SMOTE(random_state=64)\n",
    "# elif USE_SMOTETomek:\n",
    "#     logger.info('使用 SMOTETomek 混合采样')\n",
    "#     sampler = SMOTETomek(random_state=64)\n",
    "# else:\n",
    "#     assert False, '没有平衡数据'\n",
    "# X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "# logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "def objective(trial):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=1024)\n",
    "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
    "    param = {\n",
    "        'silent': True,\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "    gbm = lgb.LGBMClassifier(**param)\n",
    "    gbm.fit(train_x, train_y)\n",
    "    preds = gbm.predict(test_x)\n",
    "    # print(Counter(preds))\n",
    "    # pred_labels = np.rint(preds)\n",
    "    accuracy = sklearn.metrics.accuracy_score(test_y, preds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "print(study.best_value)\n",
    "# kf = StratifiedKFold(n_splits=10,shuffle=True,random_state=64)\n",
    "# modelcv = lgb.LGBMClassifier(**study.best_params)\n",
    "# # modelcv.save_model(f'./output/model/{datetime.now()}.json')\n",
    "# from sklearn.model_selection import cross_validate\n",
    "# cv = cross_validate(modelcv,X,y,scoring=['accuracy','precision','recall','f1'],cv=kf)\n",
    "# pprint(cv)\n",
    "# modelkf = lgb.LGBMClassifier(**study.best_trial.params)\n",
    "# for train_index, test_index in kf.split(X, y):\n",
    "#     # print(Counter(y_resampled.loc[train_index]),Counter(y_resampled.loc[test_index]))\n",
    "#     model =modelkf.fit(X.loc[train_index],y.loc[train_index])\n",
    "#     preds = model.predict(X.loc[test_index])\n",
    "#     accuracy = accuracy_score(y[test_index], preds)\n",
    "#     Accuracy.append(accuracy)\n",
    "#     precision = precision_score(y[test_index], preds)\n",
    "#     Precision.append(precision)\n",
    "#     recall = recall_score(y[test_index], preds)\n",
    "#     Recall.append(recall)\n",
    "#     f1 = f1_score(y[test_index], preds)\n",
    "#     F1.append(f1)\n",
    "#     logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "#                    f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "#     explainer = shap.TreeExplainer(model)\n",
    "#     shap_values_XGBoost_train = explainer.shap_values(X.loc[train_index])\n",
    "#     shap.summary_plot(shap_values_XGBoost_train, X.iloc[train_index])\n",
    "# logger.warning(f'accuracy\\t\\tmean:{round(np.mean(Accuracy), 3)}\\tstd:{round(np.std(Accuracy), 3)}')\n",
    "# logger.warning(f'precision\\t\\tmean:{round(np.mean(Precision), 3)}\\tstd:{round(np.std(Precision), 3)}')\n",
    "# logger.warning(f'recall\\t\\tmean:{round(np.mean(Recall), 3)}\\tstd:{round(np.std(Recall), 3)}')\n",
    "# logger.warning(f'f1\\t\\tmean:{round(np.mean(F1), 3)}\\tstd:{round(np.std(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "logger.add('./log/{time}.log')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LSVC = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LSVC:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LSVC 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 621\n",
    "xxyz = data[data['证名'] == 4]  # 547\n",
    "xqx = xqx.sample(300, random_state=64)\n",
    "xxyz = xxyz.sample(300, random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "# tsne = TSNE(n_components=2,init='pca',random_state=64)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# plt.figure(figsize=(4,4),dpi=200)\n",
    "# for i in range(0,2):\n",
    "#     plt.scatter(X_tsne[y==i][:,0],X_tsne[y==i][:,1],cmap=plt.cm.Set1(i),marker=i,label=str(i))\n",
    "# plt.legend()\n",
    "# plt.title('心气虚和心血瘀阻TSNE')\n",
    "# plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "USE_randomDownSample, USE_Tomek_links, USE_ADASYN, USE_randomOverSample, USE_SMOTE, USE_SMOTETomek = False, False, False, False, False, True\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25)\n",
    "    param = {\n",
    "        'hidden_layer_sizes': (100, 100, 100),\n",
    "        'activation': trial.suggest_categorical('activation', ['identity', 'logistic', 'tanh', 'relu']),\n",
    "        'solver': trial.suggest_categorical('solver', ['lbfgs', 'sgd', 'adam']),\n",
    "        'alpha': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'random_state': 1024\n",
    "    }\n",
    "    clf = MLPClassifier(**param)\n",
    "    clf.fit(train_x, train_y)\n",
    "    preds = clf.predict(test_x)\n",
    "    return accuracy_score(test_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=400)\n",
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "logger.add('./log/{time}.log')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LSVC = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LSVC:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LSVC 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 0]  # 621\n",
    "xxyz = data[data['证名'] == 4]  # 547\n",
    "# xqx= xqx.sample(180,random_state=64)\n",
    "# xxyz = xxyz.sample(120,random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "# tsne = TSNE(n_components=2,init='pca',random_state=64)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "# plt.figure(figsize=(4,4),dpi=200)\n",
    "# for i in range(0,2):\n",
    "#     plt.scatter(X_tsne[y==i][:,0],X_tsne[y==i][:,1],cmap=plt.cm.Set1(i),marker=i,label=str(i))\n",
    "# plt.legend()\n",
    "# plt.title('心气虚和心血瘀阻TSNE')\n",
    "# plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "USE_randomDownSample, USE_Tomek_links, USE_ADASYN, USE_randomOverSample, USE_SMOTE, USE_SMOTETomek = False, False, False, False, False, True\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.1, random_state=1024)\n",
    "    param = {\n",
    "        'C': trial.suggest_float('C', 0, 1),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    }\n",
    "    model = SVC(**param)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(test_X)\n",
    "    return accuracy_score(test_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "n_epoch = 50\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "def preprocess(path, sheet_name):\n",
    "    \"\"\"\n",
    "    对心总病数据进行预处理\n",
    "    :param path: 心总病文件路径\n",
    "    :param sheet_name: sheet的名称\n",
    "    :return: X,y\n",
    "    \"\"\"\n",
    "\n",
    "    xzb = pd.read_excel(path, sheet_name=sheet_name)\n",
    "    xzb.drop(columns=['病案号'], inplace=True)\n",
    "    xzb.drop(labels=[1742, 1741], axis=0, inplace=True)\n",
    "    xzb.drop(labels=xzb[xzb['性别'].isna()].index, inplace=True)\n",
    "    xzb = xzb.sample(frac=1).astype(int)\n",
    "    y = xzb['证名']\n",
    "    X = xzb.drop(labels=['证名', '性别', '年龄'], axis=1)\n",
    "\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "    xqx = data[data['证名'] == 0]  # 621\n",
    "    xxyz = data[data['证名'] == 4]  # 547\n",
    "    xqx['证名'] = 0\n",
    "    xxyz['证名'] = 1\n",
    "    tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "    X = tmp.drop(columns='证名')\n",
    "    y = tmp['证名']\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "class XZB(Dataset):\n",
    "    def __init__(self, X, y, train=True):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1024)\n",
    "        if train:\n",
    "            X_train.reset_index(drop=True, inplace=True)\n",
    "            y_train.reset_index(drop=True, inplace=True)\n",
    "            self.data, self.labels = X_train, y_train\n",
    "        else:\n",
    "            X_test.reset_index(drop=True, inplace=True)\n",
    "            y_test.reset_index(drop=True, inplace=True)\n",
    "            self.data, self.labels = X_test, y_test\n",
    "\n",
    "    def __len__(self):\n",
    "        # print(len(self.labels),self.labels)\n",
    "        return len(self.labels) - 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data.loc[index].values, self.labels.loc[index]\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(73, 40)\n",
    "        self.fc2 = nn.Linear(40, 20)\n",
    "        self.fc3 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = F.relu(self.fc1(input.float()))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        return F.softmax(self.fc3(out), dim=1)\n",
    "\n",
    "\n",
    "model = MLP()\n",
    "\n",
    "train_dataset = XZB(X, y, train=True)\n",
    "test_dataset = XZB(X, y, train=False)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "lossfunc = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01)\n",
    "for epoch in range(n_epoch):\n",
    "    train_loss = 0.0\n",
    "    for data, target in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = lossfunc(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    train_loss = train_loss / len(train_dataloader.dataset)\n",
    "    print('Epoch:  {}  \\tTraining Loss: {:.6f}'.format(epoch + 1, train_loss))\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_dataloader:\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print('Accuracy of the network on the test images: %d %%' % (\n",
    "            100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 心阳虚和痰蒙心神证\n",
    "import collections\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.combine import SMOTETomek\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from utils import readJSON, preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "optuna.logging.set_verbosity(optuna.logging.CRITICAL)\n",
    "from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif\n",
    "id2feature = readJSON('./input/id2feature.json')\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "sns.set(font='SimHei', font_scale=0.8)  # 解决Seaborn中文显示问题\n",
    "logger.add('./log/{time}.log')\n",
    "X, y = preprocess(path='./input/心总表.xlsx', sheet_name='总表')\n",
    "X.columns = id2feature.values()\n",
    "X.reset_index(drop=True, inplace=True)\n",
    "USE_chi2, USE_f_classif, USE_mutual_info_classif, USE_LSVC = False, False, False, True\n",
    "if USE_chi2:\n",
    "    logger.info('使用 卡方检验 进行特诊筛选')\n",
    "    chi2_model = SelectKBest(chi2, k=80)\n",
    "    X = pd.DataFrame(chi2_model.fit_transform(X, y), columns=chi2_model.get_feature_names_out())\n",
    "elif USE_f_classif:\n",
    "    logger.info('使用 F检验 进行特征筛选')\n",
    "    f_classif_model = SelectKBest(f_classif, k=80)\n",
    "    X = pd.DataFrame(f_classif_model.fit_transform(X, y), columns=f_classif_model.get_feature_names_out())\n",
    "elif USE_mutual_info_classif:\n",
    "    logger.info('使用 互信息法 进行特征筛选')\n",
    "    mutual_info_classif_model = SelectKBest(mutual_info_classif, k=80)\n",
    "    X = pd.DataFrame(mutual_info_classif_model.fit_transform(X, y),\n",
    "                     columns=mutual_info_classif_model.get_feature_names_out())\n",
    "elif USE_LSVC:\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X, y)\n",
    "    X = pd.DataFrame(X, columns=scaler.feature_names_in_)\n",
    "    lsvc = LinearSVC(C=0.01, penalty='l1', dual=False, random_state=64).fit(X, y)\n",
    "    model = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = model.transform(X)\n",
    "    col = [c for c, i in zip(X.columns, model.get_support()) if not i]\n",
    "    X.drop(columns=col, inplace=True)\n",
    "    logger.info(f'使用 LASSO 进行特征选择,剩余{X.shape[1]}个特征')\n",
    "Accuracy = []\n",
    "Precision = []\n",
    "Recall = []\n",
    "F1 = []\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "data = pd.concat([pd.DataFrame(X), pd.DataFrame(y, columns=['证名'])], axis=1)\n",
    "xqx = data[data['证名'] == 3]  # 心阳虚\n",
    "xxyz = data[data['证名'] == 6]  # 心血瘀阻\n",
    "# xqx = xqx.sample(180, random_state=64)\n",
    "# xxyz = xxyz.sample(120, random_state=64)\n",
    "xqx['证名'] = 0\n",
    "xxyz['证名'] = 1\n",
    "tmp = pd.concat([xqx, xxyz], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "X = tmp.drop(columns='证名')\n",
    "y = tmp['证名']\n",
    "\n",
    "# TSNE\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(4, 4), dpi=200)\n",
    "for i in range(0, 2):\n",
    "    plt.scatter(X_tsne[y == i][:, 0], X_tsne[y == i][:, 1], cmap=plt.cm.Set1(i), marker=i, label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心阳虚和痰蒙心神TSNE')\n",
    "plt.show()\n",
    "\n",
    "# 数据平衡\n",
    "USE_randomDownSample, USE_Tomek_links, USE_ADASYN, USE_randomOverSample, USE_SMOTE, USE_SMOTETomek = False, False, False, False, False, True\n",
    "if USE_randomDownSample:\n",
    "    logger.info('使用 randomDownSample 下采样')\n",
    "    sampler = RandomUnderSampler(random_state=64)\n",
    "elif USE_Tomek_links:\n",
    "    logger.info('使用 TomekLinks 下采样')\n",
    "    sampler = TomekLinks()\n",
    "elif USE_ADASYN:\n",
    "    logger.info('使用 ADASYN 上采样')\n",
    "    sampler = ADASYN(random_state=64)\n",
    "elif USE_randomOverSample:\n",
    "    logger.info('使用 randomOverSample 上采样')\n",
    "    sampler = RandomOverSampler(random_state=64)\n",
    "elif USE_SMOTE:\n",
    "    logger.info('使用 SMOTE 上采样')\n",
    "    sampler = SMOTE(random_state=64)\n",
    "elif USE_SMOTETomek:\n",
    "    logger.info('使用 SMOTETomek 混合采样')\n",
    "    sampler = SMOTETomek(random_state=64)\n",
    "else:\n",
    "    assert False, '没有平衡数据'\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "logger.info(f'采样前{list(Counter(y).items())},采样后{list(Counter(y_resampled).items())}')\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=64)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',\n",
    "        # 'n_estimators':trial.suggest_int('n_estimators',3500,3600),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", ['depthwise', 'lossguide']),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'dart',\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        # 'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param).fit(train_x, train_y)\n",
    "    preds = model.predict(valid_x)\n",
    "    return accuracy_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "# logger.info(study.best_value)\n",
    "logger.info(study.best_params)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=64)\n",
    "modelcv = xgb.XGBClassifier(**study.best_params)\n",
    "# modelcv.save_model(f'./output/model/{datetime.now()}.json')\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = cross_validate(modelcv, X_resampled, y_resampled, scoring=['accuracy', 'precision', 'recall', 'f1'], cv=kf)\n",
    "logger.warning(cv)\n",
    "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
    "    # print(Counter(y_resampled.loc[train_index]),Counter(y_resampled.loc[test_index]))\n",
    "    model = xgb.XGBClassifier(**study.best_trial.params).fit(X_resampled.loc[train_index], y_resampled.loc[train_index])\n",
    "    preds = model.predict(X_resampled.loc[test_index])\n",
    "    accuracy = accuracy_score(y_resampled[test_index], preds)\n",
    "    Accuracy.append(accuracy)\n",
    "    precision = precision_score(y_resampled[test_index], preds)\n",
    "    Precision.append(precision)\n",
    "    recall = recall_score(y_resampled[test_index], preds)\n",
    "    Recall.append(recall)\n",
    "    f1 = f1_score(y_resampled[test_index], preds)\n",
    "    F1.append(f1)\n",
    "    logger.warning(f'{round(np.mean(accuracy), 3)}\\t{round(np.mean(precision), 3)}\\t'\n",
    "                   f'{round(np.mean(recall), 3)}\\t{round(np.mean(f1), 3)}')\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values_XGBoost_train = explainer.shap_values(X_resampled.loc[train_index])\n",
    "    shap.summary_plot(shap_values_XGBoost_train, X_resampled.iloc[train_index])\n",
    "logger.warning(f'accuracy\\t\\tmean:{round(np.mean(Accuracy), 3)}\\tstd:{round(np.std(Accuracy), 3)}')\n",
    "logger.warning(f'precision\\t\\tmean:{round(np.mean(Precision), 3)}\\tstd:{round(np.std(Precision), 3)}')\n",
    "logger.warning(f'recall\\t\\tmean:{round(np.mean(Recall), 3)}\\tstd:{round(np.std(Recall), 3)}')\n",
    "logger.warning(f'f1\\t\\tmean:{round(np.mean(F1), 3)}\\tstd:{round(np.std(F1), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "heart = pd.read_csv('./input/heart/heart.csv')\n",
    "X = heart.drop(columns='target')\n",
    "y = heart['target']\n",
    "print(Counter(y))\n",
    "# TSNE\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决无法显示符号的问题\n",
    "tsne = TSNE(n_components=2,init='pca',random_state=64)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "plt.figure(figsize=(4,4),dpi=200)\n",
    "for i in range(0,2):\n",
    "    plt.scatter(X_tsne[y==i][:,0],X_tsne[y==i][:,1],cmap=plt.cm.Set1(i),marker=i,label=str(i))\n",
    "plt.legend()\n",
    "plt.title('心脏病')\n",
    "plt.show()\n",
    "def objective(trial):\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.1, random_state=64)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'eval_metric': 'logloss',\n",
    "        'objective': 'binary:logistic',\n",
    "        # 'n_estimators':trial.suggest_int('n_estimators',3500,3600),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 3, 12, step=1),\n",
    "        'grow_policy': trial.suggest_categorical(\"grow_policy\", ['depthwise', 'lossguide']),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        'tree_method': 'exact',\n",
    "        'booster': 'dart',\n",
    "        'gamma': trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),\n",
    "        # 'eta': trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param).fit(train_x, train_y)\n",
    "    preds = model.predict(valid_x)\n",
    "    return accuracy_score(valid_y, preds)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "print(study.best_value)\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=64)\n",
    "modelcv = xgb.XGBClassifier(**study.best_params)\n",
    "# modelcv.save_model(f'./output/model/{datetime.now()}.json')\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv = cross_validate(modelcv, X, y, scoring=['accuracy', 'precision', 'recall', 'f1'], cv=kf)\n",
    "print(cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0bc5f25abfcd0bcf916dbb7b33e019fdf2d680b5277f8a5f8149e486b88413bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
